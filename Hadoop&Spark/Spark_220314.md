

# Spark

- spark : 빅데이터 처리를 위한 오픈소스 *분석*엔진 (numpy pandas처럼 분석용)
  - standalone : spark만 사용할 때
  - yarn : 우리가 사용할거
- localhost:8088 => 들어가면 
- ![image](https://user-images.githubusercontent.com/96896873/158278025-72453b69-4175-40bd-89cd-415186b773e2.png)

- spark랑 hadoop 연결된걸 확인 가능



- lazy-evaluation : 실행을 나중에 한다. 
- spark SQL 
- spark Stream : 실시간 처리
- MLlib : 머신러닝
- GraphX : Scala에서만 가능하고 pyspark 불가. 그래프 표현할 때 사용



- spark는 scala인데 java virtual machine(jvm)기반임.

  - jvm : code는 하나지만 다양한 컴퓨터들에 각각 맞춰서 명령을 바꿔줌 (vmware와 같은 가상머신과는 완전 다른 개념)
  
- Data Structure in spark:

  - RDD : 내결함성 분산 데이터셋 (특별한 장애가 일어나도 복귀가능) , numpy의 series와 유사

  - dataset : java & scala만 가능. python은 안돼
  - dataframe : numpy의 df로 rdd가 모인 것



- 접속:

  - `pyspark`

- 나가기:

  - `exit()`

  

### Spark Grammar

#### Transformation

> input도, output도 rdd인  transformation

- `변수명=sc.range(start, end, step, partitions)`
  - ` rdd01= sc.range(0, 1000, 1, 2)` : 0~999까지 1씩 건너뛰며 2개의 파티션으로 나누기

![image](https://user-images.githubusercontent.com/96896873/158278043-19d587c0-ce50-4684-bf6f-07cc1fdac96e.png)

 :white_check_mark: 파이썬으로 작성했으나 내부적으로는 jvm을 통해서 scala로 만들어짐     

- `변수명=rdd변수.filter(조건식) `
  - `rdd02=rdd01.filter(lambda x: x%2)` : x%2가 True인 값만 반환. 즉 0이 아닌 *홀수*만 반환
  - `rdd03=rdd01.filter(lambda x: not x%2)` :  짝수만 반환
  
- `변수명=sc.parallelize(리스트, 파티션수)`

- ```python
  countries = ['korea', 'united states america', 'united kingdom', 'japan', 'france', 'germany', 'italia', 'canada', 'korea']
  countries.collect() # => list object has no attribute collect라고 오류
  
  ## list => rdd
  g8 = sc.parallelize(countries, 2) # 2개의 파티션으로 나눠서 rdd로 변환
  
  ## 결과값 :
  g8.collect()
  #['korea', 'united states america', 'united kingdom', 'japan', 'france', 'germany', 'italia', 'canada', 'korea']
  ```

- `변수명=rdd변수.distinct()` : 중복값제거

- `변수명=rdd변수.map(계산식)` : 각 요소에 해당 계산식 적용

  - `g8_upper=g8.map(lambda x: x.upper())`
  - `g8_list01=g8.map(lambda x : list(x))` : 각 요소의 값이 문자열이라면, 문자열을 list하면 각 letter가 요소가 됌
  - ![image](https://user-images.githubusercontent.com/96896873/158278067-81538efc-3caf-4e7d-87c7-615b491c5122.png)

- `변수명=rdd변수.flatMap(계산식)` : 전체에 한번에 적용
  - `g8_list02 = g8.flatMap(lambda x: list(x))` 
  - ![image](https://user-images.githubusercontent.com/96896873/158278084-4ee1aea0-6537-4c78-9aad-159cbe3c47b7.png)

- `변수명=rdd변수1.zip(rdd변수2)`: 두 변수의 같은 위치의 요소를 묶어줌 (python의 zip과 동일)

  - ```python
    counting = sc.range(1, 9, 1, 2)
    counting_g8 = counting.zip(g8)
    
    counting_g8.collect() -> [(1, 'france'), (2, 'italia'), (3, 'canada'), (4, 'japan'), (5, 'korea'), (6, 'united states america'), (7, 'united kingdom'), (8, 'germany')]
    ```

  - tuple모습으로 묶어줌

- `변수명=rdd변수.reduceByKey(계산식)` : Key별로 묶어서 value값들을 연산

  - ```python
    score = [("강호동", 10), ("유재석", 30), ("강호동", 30), ("신동엽",70), ("유재석", 60)]
    score_rdd = sc.parallelize(score, 2) # => [('강호동', 10), ('유재석', 30), ('강호동', 30), ('신동엽', 70), ('유재석', 60)]
    
    score_rdd_rbk = score_rdd.reduceByKey(lambda x, y: x + y) # 키를 기준으로 value값들을 연산
    score_rdd_rbk.collect() # => [('강호동', 40), ('신동엽', 70), ('유재석', 90)]
    ```

- `변수명=rdd변수.sortBy(식)` : 오름차순 정렬

  - ```python
    nums = sc.parallelize([1,2,3,1,1,2,5,4], 2)
    
    nums.sortBy(lambda x: x).collect() => [1, 1, 1, 2, 2, 3, 4, 5]
    ```

- `변수명=rdd변수.glom()` : 파티션 별로 grouping해줌

  - glom()은 데이터 크기가 클 때 사용 지양 ; 크기 클 때 에러가 잘나기 때문에

  - ```python
    g8.collect()
    #['korea', 'united states america', 'united kingdom', 'japan', 'france', 'germany', 'italia', 'canada', 'korea']
    
    arrs = g8.glom()
    arrs.collect()
    #[['japan', 'france', 'italia', 'canada'], ['germany', 'korea', 'united states america', 'united kingdom']]
    ```

- `변수명=rdd변수.keyBy(계산식)` 

  - `key=g8.keyBy(lambda x: x[0])` : x의 0번째 값, 즉 첫번째 문자열을 키로 하는 rdd만들기
    -  => [('j', 'japan'), ('f', 'france'), ('i', 'italia'), ('c', 'canada'), ('g', 'germany'), ('k', 'korea'), ('u', 'united states america'), ('u', 'united kingdom')]

- `변수명=rdd변수.keys()` : key만 가져오기

- `변수명=rdd변수.values()` : value만 가져오기

- `변수명=rdd변수.mapValues(계산식)` : value에만 해당 계산식 적용
  - `key.mapValues(lambda x:list(x))` 
- `변수명=rdd변수.flatMapValues(계산식)`
  - `key.flatMapValues(lambda x:list(x))`

#### Action

> input은 rdd지만 output은 rdd가 아닌 값. action은 화면에 결과를 출력하기 위한 용도로 사용

- `rdd변수.collect()` : 값 보여줌

- `rdd변수.getNumPartitions()` : Partition 개수

- `rdd변수.take(n)` : 상위 n개의 데이터 출력 (정렬된 데이터 X)

- `rdd변수.takeOrdered(n)` : 오름차순 정렬 후 상위 n개의 데이터 출력
  - `g8.takeOrdered(g8.count())` : **모든 데이터** 오름차순 정렬 후 출력 => ['canada', 'france', 'germany', 'italia', 'japan', 'korea', 'united kingdom', 'united states america']
  
- `rdd변수.top(n)` : 내림차순 정렬 후 상위 n개의 데이터 출력
  - `g8.top(g8.count())` : **모든 데이터** 내림차순 정렬 후 출력 => ['united states america', 'united kingdom', 'korea', 'japan', 'italia', 'germany', 'france', 'canada']
  
- `rdd변수.count()` : 값 개수 출력

- `rdd변수.first()` : 첫번째 값 출력 (정렬 X)

- `rdd변수.sum()` : 합한 값

- `rdd변수.max()` : 최대 값 

- `rdd변수.min()` : 최소 값

- `rdd변수.mean()` : 평균

- `rdd변수.variance()` : 분산

- `rdd변수.stdev()` : 표준편차

- `rdd변수.stats()` : 통계표 출력 (count, mean, stdev, max, min 포함)
  - `nums.stats()` => (count: 8, mean: 2.375, stdev: 1.4086784586980805, max: 5, min: 1)

- `rdd변수.countByValue()` : 값별 count해줌
  - 데이터가 많을 때 쓰는 거 지양 (count의 특징)
  - `nums.countByValue()` => defaultdict(<class 'int'>, {1: 3, 2: 2, 3: 1, 5: 1, 4: 1}) : 값이 1이 3개, 2가 2개 ..

- `rdd변수.fold(기본값, 계산식)` : 기본값으로 시작해서 파티션 단위로 계산식 수행
  - `rdd02_fold=rdd02.fold(0, lambda x, y : x + y)` : 파티션 단위로 합한 값 즉 partition1끼리 다 더하고 partition 2끼리 다 더하고 마지막으로 partition끼리 합한 값 출력 (rdd02.sum()과 같은 값. 방식만 다름)

- `rdd변수.aggregate(기본값, seqOp, combOp)` 

  - `rdd02_aggr = rdd02.aggregate(0, max, lambda x, y : x + y)` : => 1498 : 각 파티션별로 max를 찾은 뒤 찾은 값 lambda식 진행 . 왜냐하면 첫번째 partition의 가장 큰 값은 499, 두번째 파티션의 가장 큰 값은 999였기 때문에

- `rdd변수.reduce(계산식)` : 데이터 병렬 연산 (두개씩 가져와서 연산)

  - `rdd02_reduce = rdd02.reduce(lambda x,y: x + y)` => 25000 : 데이터를 병렬 연산방식으로 진행 (결과는 rdd02.sum()과 동일)

  - ```python
    # g8에서 가장 긴 단어 찾기
    def g8Max(x,y):
    	if len(x) > len(y):
    		return x
    	else:
    		return y
    		
    g8_max_length = g8.reduce(g8Max)
    g8_max_length #=> 'united states america'
    ```

  - :white_check_mark: 함수 선언 끝나면 엔터 두번 쳐서 줄을 바꿔야지 컴퓨터가 함수 끝난 걸알고 계속 진행 가능

- 실습 :
  - g8에서 가장 긴 단어 찾기
  
    ```python
    # g8에서 가장 짧은 단어 찾기
    g8_min_length = g8.reduce(lambda x,y : x if len(x) < len(y) else y)
    
    g8_min_length # => 'korea' (여러개라면 하나의 값만 나옴)
    ```





- 텍스트파일로 저장하기 :
  - `rdd변수.saveAsTextFile("경로")`
    - `g8.saveAsTextFile("/tmp/g8")`
    - :white_check_mark: saveAsTextFile은 파티션 별로 저장 됌!!!!! 예) part-0001, part-0002..

- 텍스트파일 불러오기 :

  - `변수명=sc.textFile("경로")`

    - `result=sc.textFile("/tmp/g8/part-000*")`  :part-000으로 시작하는 모든 파일들을 한 결과값으로 가져와줌
    - `result.collect()` => ['france', 'italia', 'canada', 'japan', 'korea', 'united states america', 'united kingdom', 'germany']

    

=> 그렇다면 /tmp/g8의 위치에 part-000*가 있는지 확인하러 가보자!!

`exit()` -> `cd tmp` -> `ls` : tmp에 g8이 없음 



=> tmp폴더라 그런걸까? 확인하러 가보자

`pyspark` =>`result = sc.textFile("/tmp/g8/part-000*")` =>`result.collect()` : pyspark 껐다켰는데도 그대로 남아있음



=> 정답은 hdfs에 저장되어있음! spark에서 가져오거나 저장은 hadoop(hdfs)에서 함

#### HDFS 명령어

- `hdfs dfs -명령어` 형식으로 작성
- `hdfs dfs -ls /` : /밑에 ls하기

![image](https://user-images.githubusercontent.com/96896873/158278104-69721c7f-f190-4b60-9d80-7ad40f0aa555.png)

- `hdfs dfs -ls /tmp/g8/` : 파티션별로 파일에 저장 되는 것 확인 가능

- `hdfs dfs -cat /tmp/g8/part-00000` 

- `hdfs dfs -cat /tmp/g8/part-00001`

![image](https://user-images.githubusercontent.com/96896873/158278115-39dde15f-3582-45f4-9d0b-1c59eb55dbd9.png)

- `hdfs dfs -mkdir -p /home/jisu` : 원래 dir는 한번에 dir 하나밖에 못만드는데(즉 dir안에 dir 생성 불가능) *-p*를 활용하면 home도 만들고 그 안에 jisu라는 폴더도 만들기 가능



- hdfs 직접 확인하기 :

  - localhost:9870 접속
  - ![image](https://user-images.githubusercontent.com/96896873/158278130-f6c12d03-f0f9-4194-929d-b00fc2b9593f.png)

  - utilities -> browse the file system 
  - ![image](https://user-images.githubusercontent.com/96896873/158278149-c43d2f95-22da-4be7-8828-d61070bc7f0b.png)

  - 생성했던 home폴더가 정상적으로 만들어져 있음!
  - ![image](https://user-images.githubusercontent.com/96896873/158278171-d495fc46-0a22-41b0-9ca6-c3b6271f0934.png)

  - /tmp/g8 안에 part-00000과 part-00001이 잘만들어진 것도 확인 가능





---

##### Key 연산

- `pyspark` 

- 앞서 spark에서 작업한 결과물을 저장한게 아니라면 다 휘발되었기 때문에 key라는 변수 다시 생성 :

- ```python
  countries = ['korea', 'united states america', 'united kingdom', 'japan', 'france', 'germany', 'italia', 'canada', 'korea']
  g8 = sc.parallelize(countries, 2)
  g8 = g8.distinct()
  
  key = g8.keyBy(lambda x: x[0])
  key.collect() #=> [('j', 'japan'), ('f', 'france'), ('i', 'italia'), ('c', 'canada'), ('g', 'germany'), ('k', 'korea'), ('u', 'united states america'), ('u', 'united kingdom')]
  ```

- `key.keys().collect()` : key값만 가져오기

- `key.values().collect()` : value값만 가져오기

- `key.mapValues(lambda x:list(x)).collect()`  => [('j', ['j', 'a', 'p', 'a', 'n']), ('f', ['f', 'r', 'a', 'n', 'c', 'e']), ('i', ['i', 't', 'a', 'l', 'i', 'a']), ('c', ['c', 'a', 'n', 'a', 'd', 'a']), ('g', ['g', 'e', 'r', 'm', 'a', 'n', 'y']), ('k', ['k', 'o', 'r', 'e', 'a']), ('u', ['u', 'n', 'i', 't', 'e', 'd', ' ', 's', 't', 'a', 't', 'e', 's', ' ', 'a', 'm', 'e', 'r', 'i', 'c', 'a']), ('u', ['u', 'n', 'i', 't', 'e', 'd', ' ', 'k', 'i', 'n', 'g', 'd', 'o', 'm'])] : mapValue는 value에 적용

- `key.flatMapValues(lambda x:list(x)).collect() `=> [('j', 'j'), ('j', 'a'), ('j', 'p'), ('j', 'a'), ('j', 'n'), ('f', 'f'), ('f', 'r'), ('f', 'a'), ('f', 'n'), ('f', 'c'), ('f', 'e'), ('i', 'i'), ('i', 't'), ('i', 'a'), ('i', 'l'), ('i', 'i'), ('i', 'a'), ('c', 'c'), ('c', 'a'), ('c', 'n'), ('c', 'a'), ('c', 'd'), ('c', 'a'), ('g', 'g'), ('g', 'e'), ('g', 'r'), ('g', 'm'), ('g', 'a'), ('g', 'n'), ('g', 'y'), ('k', 'k'), ('k', 'o'), ('k', 'r'), ('k', 'e'), ('k', 'a'), ('u', 'u'), ('u', 'n'), ('u', 'i'), ('u', 't'), ('u', 'e'), ('u', 'd'), ('u', ' '), ('u', 's'), ('u', 't'), ('u', 'a'), ('u', 't'), ('u', 'e'), ('u', 's'), ('u', ' '), ('u', 'a'), ('u', 'm'), ('u', 'e'), ('u', 'r'), ('u', 'i'), ('u', 'c'), ('u', 'a'), ('u', 'u'), ('u', 'n'), ('u', 'i'), ('u', 't'), ('u', 'e'), ('u', 'd'), ('u', ' '), ('u', 'k'), ('u', 'i'), ('u', 'n'), ('u', 'g'), ('u', 'd'), ('u', 'o'), ('u', 'm')] :

- `key.groupByKey()` => [('j', ('japan')], ... ('u', ('united states america','united kingdom'))]

- `key.groupByKey().mapValues(lambda x: list(x)).collect()` => [('g', ['germany']), ('j', ['japan']), ('i', ['italia']), ('c', ['canada']), ('f', ['france']), ('k', ['korea']), ('u', ['united states america', 'united kingdom'])] : 문자열이 아니라 ()에들어있기 때문에 list(x)해도 문자열끼리 되지 않음!

- `key.groupByKey().mapValues(lambda x: len(x)).collect()` => [('g', 1), ('i', 1), ('c', 1), ('j', 1), ('f', 1), ('k', 1), ('u', 2)] : value개수 세줌

- `key.reduceByKey(lambda x,y : x + "," + y).collect()`=> [('g', 'germany'), ('i', 'italia'), ('c', 'canada'), ('j', 'japan'), ('f', 'france'), ('k', 'korea'), ('u', 'united states america,united kingdom')]

- `key.countByKey()` : key별로 개수 새기 => defaultdict(<class 'int'>, {'f': 1, 'i': 1, 'c': 1, 'j': 1, 'k': 1, 'u': 2, 'g': 1})

- `exit()`

---

#### 실습

- data.zip 파일 다운로드
- 파일 폴더 열어서 Home안에 data.zip 넣기



- `mkdir data` : 홈디렉토리에서 data라는 dir만듬

- `unzip data.zip -d ./data` : 생성한 data라는 dir안에 data.zip을 unzip함 

- `cd data`

- `ls` => flights  retails  shakespeare.txt  simple-ml : 잘 압축해제되어서 풀어져 들어가 있음



- `cd` : 홈디렉토리로 이동



- 일반 ubuntu local에 있는 걸 hdfs로 복사해서 저장 :
  - `hdfs dfs -put {파일local 위치} {새로운 hdfs 위치}`
    - `hdfs dfs -put /home/jisu/data /home/jisu/data` 

- `hdfs dfs -ls /home/jisu/data` : 정상적으로 hdfs에 저장!

---

pyspark 접속 :

- `pyspark`

- `result = sc.textFile("/tmp/g8/part-000*")`

- `g8 = result.distinct()`

- `g8_upper = g8.map(lambda x: x.upper())`

- `g8_upper.collect()` => ['FRANCE', 'JAPAN', 'UNITED STATES AMERICA', 'UNITED KINGDOM', 'ITALIA', 'CANADA', 'KOREA', 'GERMANY']



- loalhost:8088 접속

- ![image](https://user-images.githubusercontent.com/96896873/158278192-6afa1069-a79b-4c69-8669-89cc5da25d3d.png)

  - scheduler 클릭
  - ![image](https://user-images.githubusercontent.com/96896873/158278211-f7a7e7aa-138c-42c3-be96-2ea134c59a63.png)
  - tracking UI 밑에 ApplicationMaster 클릭
  - ![image](https://user-images.githubusercontent.com/96896873/158278225-9bc2c401-ddf9-4d77-a762-2a8ff74a512d.png)
  - Event timeline 클릭 
  - 우리가 하는 작업들 확인 가능

  

  - 상단에 Stages 클릭 : 우리가 방금 pyspark에서 distinct와 collect명령어 사용한 로그들이 적혀 있음
  - ![image](https://user-images.githubusercontent.com/96896873/158278236-413a72e9-509b-4fd2-9c8f-64ce3c11e5c2.png)
  - distinct as \<stdin>:1 클릭
  - ![image](https://user-images.githubusercontent.com/96896873/158278250-cba59b73-c248-4d0d-99ba-60a67b64a7de.png)
  - DAG visualization 클릭 
  - 단계 하나하나가 계보(리니지)

`exit()`

---

- `vim spark_wc.py` -> spark_wc.py가 없지만 없는 파일 vim하면 없는 파일도 생성

```python
import sys, re
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('Word Count')
sc = SparkContext(conf=conf)

if (len(sys.argv) != 3):
    print("wordcount.py input_file output_dir 형태로 실행해 주세요")
    sys.exit(0)
else:
    inputfile = sys.argv[1]
    outputdir = sys.argv[2]
    
wordcount = sc.textFile(inputfile)\
           .repartition(10)\
           .filter(lambda x: len(x) > 0)\
            .flatMap(lambda x: re.split('\W+', x))\
             .filter(lambda x: len(x) > 0)\
              .map(lambda x:(x.lower(), 1))\
              .reduceByKey(lambda x, y: x + y)\
              .map(lambda x:(x[1], x[0]))\
              .sortByKey(ascending=False)\
              .persist()

wordcount.saveAsTextFile(outputdir)
top10 = wordcount.take(5)
result = []
for counts in top10:
    result.append(counts[1])
print(result)
```

- `spark-submit spark_wc.py ~/data/shakespeare.txt ~/result` : spark_wc.py실행해서 shakespeare.txt를 input_file로 result라는 directory만들어서 저장

- `hdfs dfs -cat ~/result/part-00000` => 어떤 단어가 몇개씩 나왔는지 partiton별로 저장된거 화면에 출력

![image](https://user-images.githubusercontent.com/96896873/158278262-59c212d5-1d18-4019-88d1-1340c67b8aca.png)

- partition이 총 10개로 나뉘어서 저장되었기 때문에 part-0001 ~ part-0009까지 내림차순으로 적혀있음
- `stop-all.sh` 

---

- 코드 설명 :

```python
import sys, re
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName('Word Count')
# SparkContext : 스파크 클러스터의 연결
# SparkContext 사용 때는 master와 appname 꼭 지정해야함 (conf나 parameter사용해서)
sc = SparkContext(conf=conf)

# sys.argv = spark-submit에 작성된 인자들
if (len(sys.argv) != 3):
    print("wordcount.py input_file output_dir 형태로 실행해 주세요")
    sys.exit(0)
else:
    inputfile = sys.argv[1]
    outputdir = sys.argv[2]
    
# sc.textFile() : HDFS에서 textfile 읽어오기
wordcount = sc.textFile(inputfile)\
			# 다시 partition
           .repartition(10)\
       		# 아무것도 없는게 아닐 때 (공백 데이터 제거): 
           .filter(lambda x: len(x) > 0)\
        	# re.split() : x를 정규식인데 영문자 or 숫자가 아닌 것이 1개 이상인 것 기준으로 나눠라 (느낌표, 띄어쓰기)
            .flatMap(lambda x: re.split('\W+', x))\
             .filter(lambda x: len(x) > 0)\
                # map(lambca x:(x.lower(), 1) : (단어, 1) 이런 모양
              .map(lambda x:(x.lower(), 1))\
            	# reduceByKEY() : 동일한 key끼리 묶어서 값들 합산하는 연산 수행 => ('f', 100)
              .reduceByKey(lambda x, y: x + y)\
            	# map(lambda x:(x[1], x[0])) => (100, 'f')
              .map(lambda x:(x[1], x[0]))\
              .sortByKey(ascending=False)\
                # 데이터 저장 with default storage-level(메모리)로
              .persist()

wordcount.saveAsTextFile(outputdir)
top10 = wordcount.take(5)
result = []
for counts in top10:
    result.append(counts[1])
print(result)
```





## 기타

- 모든 작업이 끝나면 `stop-all.sh` 꼭 하기 : 안하면 렉걸릴 지도

- softlink 깨져서 파란색이 아니라 빨간색으로 뜬다면 깨진 softlink 삭제(rm -rf hadoop)하고 다시`ln -s hadoop-3.3.1 hadoop`로 생성하기 

- bin 파일 밑에는 실행파일 있음
- 갑자기 vim하다가 먹통되면, 내가 있었던 마지막 위치에서 작업하던 해당 파일이름의 .swp파일을 찾아서 작업
  - 수정한게 예를 들어 hdfs-site.xml 파일이라면 hdfs-site.swp파일 수정하면 됌.

-   https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html <- 이거는 single이 아니라 분산 node일 때 사용. 시도해보기

- ctrl + l : clear
